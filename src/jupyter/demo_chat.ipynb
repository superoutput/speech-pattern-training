{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFyIdzo73l5M"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gnoparus/bualabs/blob/master/nbs/15b-numerical-operations-tensor.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19S-d0yBdfhl"
      },
      "source": [
        "- ติดตั้ง lib pythainlp เพื่อใช้ในการตัดคำ และ อัพเดท lib gensim\n",
        "- ทำการ mount google drive หรือ get dataset จาก git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC6doG-MWeOb",
        "outputId": "98a43444-715a-47d7-bf25-84b7f04d8c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'speech-pattern-training'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 36 (delta 8), reused 28 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (36/36), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pythainlp\n",
            "  Downloading pythainlp-3.1.1-py3-none-any.whl (9.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Installing collected packages: pythainlp\n",
            "Successfully installed pythainlp-3.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/superoutput/speech-pattern-training.git\n",
        "!pip install pythainlp\n",
        "!pip install gensim -U\n",
        "!pip install matplotlib\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH5PC3Zud_sk"
      },
      "source": [
        "[Document Referance](https://medium.com/@newnoi/%E0%B8%AA%E0%B8%AD%E0%B8%99%E0%B8%84%E0%B8%AD%E0%B8%A1%E0%B8%9E%E0%B8%B9%E0%B8%94%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B9%84%E0%B8%97%E0%B8%A2%E0%B9%86-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-machine-learning-model-part2-2a1609af1bd7) สำหรับการทดสอบ\n",
        "\n",
        "1. เตรียม Word Embedding model ภาษาไทย โดยใช้ gensim library ในการเทรน Word2Vec และนำ Corpus  มาจากบทความเกี่ยวกับโรคไข้หัดแมว(ข้อมูลส่วนนี้น้อยมากๆ ซึ่งทำให้ประสิทธิ์ภาพไม่ดี)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTFaIKElylsc"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from gensim.models.word2vec import LineSentence\n",
        "\n",
        "def preprocess(input_path, output_path):\n",
        "  inputfile = open(input_path,'r')\n",
        "  output = open(output_path, 'w')\n",
        "  i = 0\n",
        "  for line in inputfile.readlines():\n",
        "    text = word_tokenize(line, keep_whitespace=False)\n",
        "    output.write(' '.join(text) + \"\\n\")\n",
        "    i += 1\n",
        "    if (i % 100 == 0) or (i <= 10 ):\n",
        "        print(\"Saved \" + str(i) + \" articles\")\n",
        "  output.close()\n",
        "  print(\"Finished Saved \" + str(i) + \" articles\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  inp = \"/content/drive/MyDrive/Colab Notebooks/corpus.txt\"\n",
        "  outp = \"/content/word.th.text\"\n",
        "  preprocess(inp, outp)\n",
        "  model = Word2Vec(LineSentence(outp), vector_size=10, window=10, min_count=1,workers=multiprocessing.cpu_count())\n",
        "  model.save('word_th.model')  \n",
        "  print(\"Finished Saved Word Model\")\n",
        "  # wv_model = Word2Vec.load(\"/content/word_th.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq3yoFJ0lSND"
      },
      "source": [
        "หรืออีกวิธีสามารถ get word2vec จาก model thai2fit_wv [เลือกทดสอบด้วย model นี้]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhNQ33DCVbn6"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "from pythainlp.corpus import get_corpus_path\n",
        "\n",
        "_MODEL_NAME = \"thai2fit_wv\"\n",
        "\n",
        "\n",
        "def get_model() -> Word2VecKeyedVectors:\n",
        "    \"\"\"\n",
        "    Get word vector model.\n",
        "\n",
        "    :return: `gensim` word2vec model\n",
        "    :rtype: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
        "    \"\"\"\n",
        "    path = get_corpus_path(_MODEL_NAME)\n",
        "    return KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "\n",
        "\n",
        "\n",
        "wv_model = get_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0lxT2T6dbAp"
      },
      "source": [
        "2. การ train model \n",
        "- เตรียมข้อมูลเพื่อนำไป Train โดยข้อมูลที่ใช้เทรนจะเป็นประโยคที่เราจะสอนให้ตอบ\n",
        "- สร้าง Encoder-Decoder LSTM model\n",
        "- Train model โดยนำข้อมูลเข้าไปเรียนรู้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VbvHRNzFvN5e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import Input,Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from gensim.corpora import WikiCorpus\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI5uBXSWnNSJ"
      },
      "source": [
        "function load_data โดย return question_list คือคำถาม, answer_list คือคำตอบ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JQEiKWJA41LE"
      },
      "outputs": [],
      "source": [
        "def load_data(datafile): \n",
        "    data= pd.read_csv(datafile)\n",
        "    question_list = [i.replace('\\n', '') for i in data['2']]\n",
        "    answer_list = [i.replace('\\n', '') for i in data['3']]\n",
        "    return question_list,answer_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-tEmRhNnp_l"
      },
      "source": [
        "function preparingword จะทำการตัดคำในแต่ละประโยค"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gHXogMr54CPx"
      },
      "outputs": [],
      "source": [
        "def preparingword(listword):\n",
        "    word =[]\n",
        "    for w in listword:\n",
        "        word.append(word_tokenize(w,keep_whitespace=False))\n",
        "    return word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_CmzAEushYS"
      },
      "source": [
        "function max_len นับจำนวน word ในประโยคที่เยอะที่สุดฃอง dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DSVAZDVsglc"
      },
      "outputs": [],
      "source": [
        "def max_len(q, a):\n",
        "    new_data = [len(i) for i in q]\n",
        "    new_data.extend([len(i) for i in a])\n",
        "    return max(new_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl-QlgBGn9kv"
      },
      "source": [
        "function padding_sequence จะทำการเพิ่มให้ทุกประโยค มีจำนวน word เท่ากัน โดยการเติม `<EOS>`ให้ครบตาม maxlen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cjWQK0AT4Ggk"
      },
      "outputs": [],
      "source": [
        "def padding_sequence(listsentence, maxseq):\n",
        "    dataset = []\n",
        "    for s in listsentence:\n",
        "        n = maxseq - len(s)\n",
        "        if n>0:\n",
        "            eos = [\"<EOS>\"]*n\n",
        "            s.extend(eos)\n",
        "            dataset.append(s)\n",
        "        elif n<0:\n",
        "            dataset.append(s[0:maxseq])\n",
        "        else:\n",
        "            dataset.append(s)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu2ZzfRFoxxc"
      },
      "source": [
        "function word_index จะทำการหาค่าของ word vector กรณีไม่พบ word และ similar word ในwv_model จะทำการ default ให้เป็น `<eos>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9a1i-dPJ4Gtv"
      },
      "outputs": [],
      "source": [
        "def word_index(listword):\n",
        "    dataset = []\n",
        "    for sentence in listword:\n",
        "        tmp = []\n",
        "        for w in sentence:\n",
        "            tmp.append(word2idx(w))\n",
        "        dataset.append(tmp)\n",
        "    return np.array(dataset)\n",
        "\n",
        "def word2idx(word):\n",
        "    index = 0\n",
        "    try:\n",
        "        index = wv_model.key_to_index[word] \n",
        "    except:\n",
        "        try:\n",
        "            sim = similar_word(word)\n",
        "            index = wv_model.key_to_index[sim] \n",
        "        except:\n",
        "            index = wv_model.key_to_index[\"<eos>\"] \n",
        "    return index\n",
        "\n",
        "def similar_word(word):\n",
        "    sim_word = wv_model.most_similar(word)\n",
        "    try:\n",
        "        return sim_word[0]\n",
        "    except:\n",
        "        return \"<eos>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g9h4_jDpJ8K"
      },
      "source": [
        "function embedding_model จะทำการ define model โดยสร้าง Encoder-Decoder LSTM model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wmu-yuJu4HXM"
      },
      "outputs": [],
      "source": [
        "def embedding_model():\n",
        "    # define word embedding\n",
        "    vocab_list = [(k, wv_model[k]) for k,v in wv_model.key_to_index.items()]\n",
        "    embeddings_matrix = np.zeros((len(wv_model.key_to_index.items()) + 1, wv_model.vector_size))\n",
        "    for i in range(len(vocab_list)):\n",
        "        word = vocab_list[i][0]\n",
        "        embeddings_matrix[i + 1] = vocab_list[i][1]\n",
        "\n",
        "    embedding_layer = Embedding(input_dim=len(embeddings_matrix),\n",
        "                                output_dim=300,\n",
        "                                weights=[embeddings_matrix],\n",
        "                                trainable=False,\n",
        "                                name=\"Embedding\"\n",
        "                                )\n",
        "    return embedding_layer,len(embeddings_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TaKvlBUI4HgC"
      },
      "outputs": [],
      "source": [
        "def ende_embedding_model(n_input, n_output, n_units):\n",
        "    encoder_inputs = Input(shape=(None,), name=\"Encoder_input\")\n",
        "\n",
        "    encoder = LSTM(n_units,return_state=True, name='Encoder_lstm')\n",
        "    Shared_Embedding,vocab_size = embedding_model()\n",
        "    word_embedding_context = Shared_Embedding(encoder_inputs)\n",
        "    encoder_outputs, state_h, state_c = encoder(word_embedding_context)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,), name=\"Decoder_input\")\n",
        "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True, name=\"Decoder_lstm\")\n",
        "    word_embedding_answer = Shared_Embedding(decoder_inputs)\n",
        "    decoder_outputs, _, _ = decoder_lstm(word_embedding_answer, initial_state=encoder_states)\n",
        "    decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax', name=\"Dense_layer\"))\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "    decoder_state_input_h = Input(shape=(n_units,), name=\"H_state_input\")\n",
        "    decoder_state_input_c = Input(shape=(n_units,), name=\"C_state_input\")\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(word_embedding_answer, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "    return model, encoder_model, decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv8IVgGy4tYj"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # data = \"/content/drive/MyDrive/Colab Notebooks/train_qa.txt\"\n",
        "    data = \"/content/drive/MyDrive/dataset/itqa2.csv\"\n",
        "    max_word=10\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    X1 = []\n",
        "    X2 = []\n",
        "    Y = []\n",
        "\n",
        "    # load_data x คือคำถาม, y คือคำตอบ\n",
        "    dataX,dataY = load_data(data)\n",
        "\n",
        "    # preparingword และตัดคำ\n",
        "    dataX = preparingword(dataX)\n",
        "    dataY = preparingword(dataY)\n",
        "    \n",
        "    max_word = max_len(dataX, dataY)\n",
        "\n",
        "    for sentence in dataX:\n",
        "        X1.append(sentence)\n",
        "    for sentence in dataY:\n",
        "        Y.append(sentence)\n",
        "    for sentence in dataY:\n",
        "        add_start_word = ['_']\n",
        "        add_start_word.extend(sentence)\n",
        "        X2.append(add_start_word)\n",
        "\n",
        "    # padding_sequence โดยเพิ่มให้ทุก sentence มีจำนวน word เท่ากัน\n",
        "    X1 = padding_sequence(X1, maxlen)\n",
        "    X2 = padding_sequence(X2, maxlen)\n",
        "    Y = padding_sequence(Y, maxlen)\n",
        "\n",
        "    # word_index หาค่าของ word vector\n",
        "    X1 = word_index(X1)\n",
        "    X2 = word_index(X2)\n",
        "    Y = word_index(Y)\n",
        "\n",
        "    # ทำ One-hot Encode ตามจำนวนของ word\n",
        "    Y = to_categorical(Y, num_classes=51359)\n",
        "    \n",
        "    # define model\n",
        "    train, infenc, infdec = ende_embedding_model(maxlen, maxlen, 256)\n",
        "    train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "    # train model\n",
        "    train.fit([X1, X2], Y, epochs=300, batch_size=32)\n",
        "\n",
        "    infenc.save_weights(\"/content/drive/MyDrive/model/model_enc_weight.h5\")\n",
        "    infenc.save(\"/content/drive/MyDrive/model/model_enc.h5\")\n",
        "    print(\"Saved model to disk\")\n",
        "    infdec.save_weights(\"/content/drive/MyDrive/model/model_dec_weight.h5\")\n",
        "    infdec.save(\"/content/drive/MyDrive/model/model_dec.h5\")\n",
        "    print(\"Saved model to disk\")\n",
        "    return train\n",
        "\n",
        "model_his = train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YZNDbP1U8DM"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(model_his.history.history['acc'], label='acc')\n",
        "plt.plot(model_his.history.history['loss'], label='loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRjH4GwngKsK"
      },
      "source": [
        "3. นำ Model ที่ได้มาใช้ โดยการป้อนคำถามเข้าไป"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BeK7aKZnKCY"
      },
      "outputs": [],
      "source": [
        "def test_model():\n",
        "    # # define model\n",
        "    train, infenc, infdec = ende_embedding_model(maxlen, maxlen, 256)\n",
        "\n",
        "    # load weights\n",
        "    infenc.load_weights(\"/content/drive/MyDrive/model/model_enc.h5\")\n",
        "    infdec.load_weights(\"/content/drive/MyDrive/model/model_dec.h5\")\n",
        "\n",
        "    # start prediction\n",
        "    while True:\n",
        "        input_data = input()\n",
        "        if input_data == 'end':\n",
        "          break\n",
        "        input_data = preparingword([input_data])\n",
        "        input_data = padding_sequence(input_data, maxlen)\n",
        "        input_data = word_index(input_data)\n",
        "        target = predict_sequence(infenc, infdec, input_data, maxlen, 10)\n",
        "        int_target = onehot_to_int(target)\n",
        "        ans = invert(int_target)\n",
        "        print(ans)\n",
        "    return 0\n",
        "\n",
        "def onehot_to_int(inputvector):\n",
        "    return np.argmax(inputvector, axis=1)\n",
        "\n",
        "def invert(inputlist):\n",
        "    sentence = []\n",
        "    for w in inputlist:\n",
        "        sentence.append(wv_model.index_to_key[w])\n",
        "    return (\"\".join(sentence).replace(\"<eos>\",\" \"))\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
        "    # encode\n",
        "    state = infenc.predict(source)\n",
        "    # start of sequence input\n",
        "    target_seq = np.array(word_index(\"_\"))\n",
        "    # collect predictions\n",
        "    output = list()\n",
        "    for t in range(n_steps):\n",
        "        # predict next char\n",
        "        yhat, h, c = infdec.predict([target_seq] + state)\n",
        "        # store prediction\n",
        "        output.append(yhat[0,0,:])\n",
        "        # update state\n",
        "        state = [h, c]\n",
        "        # update target sequence\n",
        "        target_seq = np.array([[np.argmax(yhat[0,0,:])]])\n",
        "    return np.array(output)\n",
        "\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z9SDeAjgVCv"
      },
      "source": [
        "note... ใช้รันทดสอบ code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srDKmfWmca6U"
      },
      "outputs": [],
      "source": [
        "# # for test code\n",
        "# from gensim import models\n",
        "# wv_model = models.Word2Vec.load(\"/content/drive/MyDrive/Colab Notebooks/word_th.model\")\n",
        "# print(wv_model.wv.key_to_index.items())\n",
        "# print(wv_model.wv.index_to_key[2])\n",
        "aa = wv_model.most_similar('อีเมล')\n",
        "print(aa)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
